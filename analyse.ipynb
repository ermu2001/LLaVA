{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union, List\n",
    "import copy\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "import transformers\n",
    "\n",
    "from llava.mm_utils import process_images, load_image_from_base64, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.mm_utils import process_images, load_image_from_base64, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "\n",
    "\n",
    "from attention_analyse import (\n",
    "    load_image,\n",
    "    transpose_list,\n",
    "    make_image_grid,\n",
    "    visualize_1d_energy,\n",
    "    visualize_spatial_energy\n",
    ")\n",
    "device='cuda:1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ARGS():\n",
    "    model_path=\"liuhaotian/llava-v1.5-7b\"\n",
    "    model_base=None\n",
    "    model_name=None\n",
    "    device=device\n",
    "    multi_modal=True\n",
    "    load_8bit=False\n",
    "    load_4bit=False\n",
    "args = ARGS() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyser():\n",
    "    is_multimodal=True\n",
    "    def __init__(self, args):\n",
    "        self.set_args(args)\n",
    "        \n",
    "    def set_args(self, args):\n",
    "        self.args = args\n",
    "        model_path = args.model_path\n",
    "        if model_path.endswith(\"/\"):\n",
    "            model_path = model_path[:-1]\n",
    "\n",
    "        if args.model_name is None:\n",
    "            model_paths = model_path.split(\"/\")\n",
    "            if model_paths[-1].startswith('checkpoint-'):\n",
    "                self.model_name = model_paths[-2] + \"_\" + model_paths[-1]\n",
    "            else:\n",
    "                self.model_name = model_paths[-1]\n",
    "        else:\n",
    "            self.model_name = args.model_name\n",
    "        self.device = self.args.device\n",
    "    \n",
    "    def load_from_args(self):\n",
    "        model_path = self.args.model_path\n",
    "        model_base = self.args.model_base\n",
    "        load_8bit = self.args.load_8bit\n",
    "        load_4bit = self.args.load_4bit\n",
    "        model_name = self.model_name\n",
    "        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\n",
    "            model_path, model_base, model_name, load_8bit, load_4bit, device=self.device)\n",
    "        self.is_multimodal = 'llava' in self.model_name.lower()\n",
    "\n",
    "    def set_modules(self, model=None, tokenizer=None, image_processor=None):\n",
    "        self.model = model if model is not None else self.model\n",
    "        self.tokenizer = tokenizer if tokenizer is not None else self.tokenizer\n",
    "        self.image_processor = image_processor if image_processor is not None else self.image_processor\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def analyse_attention(self, start_energy, attentions):\n",
    "        from_seq_len = start_energy.shape[2]\n",
    "        # energy_transforme_matrix = torch.diag(torch.ones(to_seq_len))\n",
    "        # energy_transforme_matrix = torch.unsqueeze(energy_transforme_matrix, 0)\n",
    "        # print(energy_transforme_matrix.shape)\n",
    "        # return\n",
    "        energies = []\n",
    "        energy = start_energy.to(dtype=torch.float32)\n",
    "        for layer_attention in attentions:\n",
    "            layer_attention = layer_attention.to(dtype=energy.dtype)\n",
    "            layer_attention = layer_attention.mean(1) # mean over multi heads\n",
    "            \n",
    "            if layer_attention.shape[1] != from_seq_len:\n",
    "                raise ValueError('')\n",
    "            \n",
    "            # print(layer_attention.shape)\n",
    "            # print(layer_attention.sum(2)) # all one\n",
    "            # print(energy.shape)\n",
    "            energy = energy @ layer_attention\n",
    "            energies.append(energy)\n",
    "            energy[:, :, 0] = 0\n",
    "            energy = energy / energy.sum(2) \n",
    "            # mean, std = energy.mean(2, keepdim=True), energy.std(2, keepdim=True)\n",
    "            # min_, max_ = mean-3*std, mean+3*std\n",
    "            # print(min_, max_)\n",
    "            # energy = energy.clip(min_, max_)\n",
    "            # print(energy)\n",
    "            # energy = torch.softmax(energy , dim=2) # soft arg max\n",
    "            # print(energy)\n",
    "            # energy = (energy - energy.mean(1)) / energy.std() + energy.mean(1)\n",
    "            # print(energy.shape)\n",
    "            # print('sum to 1:', energy.sum(2))\n",
    "        return energies\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def run4attention(self, input_ids, images):\n",
    "        tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor\n",
    "        images = process_images(images, image_processor, model.config)\n",
    "        if type(images) is list:\n",
    "            images = [image.to(self.model.device, dtype=torch.float16) for image in images]\n",
    "        else:\n",
    "            images = images.to(self.model.device, dtype=torch.float16)\n",
    "    \n",
    "        lm_model_out = model(\n",
    "            input_ids=input_ids,\n",
    "            images=images,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "        lm_attentions = lm_model_out.attentions\n",
    "        \n",
    "        vision_tower = model.get_vision_tower().vision_tower\n",
    "        vision_model_out = vision_tower(images, output_attentions=True)\n",
    "        vision_attentions = vision_model_out.attentions\n",
    "        return lm_attentions, vision_attentions\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, query, images, **params):\n",
    "        tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor\n",
    "        \n",
    "        # using conversation_lib to preprocess the text input\n",
    "        conv = conversation_lib.conv_llava_v1.copy()\n",
    "        user_input, assistant_output, target_text = query\n",
    "        conv.append_message(conv.roles[0], (user_input, images[0]))\n",
    "        # conv.append_message(conv.roles[1], assistant_output)\n",
    "        prompt = conv.get_prompt()\n",
    "        ori_prompt = prompt\n",
    "        num_image_tokens = 0\n",
    "        if images is not None and len(images) > 0 and self.is_multimodal:\n",
    "            if len(images) > 0:\n",
    "                if len(images) != prompt.count(DEFAULT_IMAGE_TOKEN):\n",
    "                    raise ValueError(\"Number of images does not match number of <image> tokens in prompt\")\n",
    "\n",
    "                images = [load_image(image) for image in images]\n",
    "                images = process_images(images, image_processor, model.config)\n",
    "\n",
    "                if type(images) is list:\n",
    "                    images = [image.to(self.model.device, dtype=torch.float16) for image in images]\n",
    "                else:\n",
    "                    images = images.to(self.model.device, dtype=torch.float16)\n",
    "\n",
    "                replace_token = DEFAULT_IMAGE_TOKEN\n",
    "                if getattr(self.model.config, 'mm_use_im_start_end', False):\n",
    "                    replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "                prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "\n",
    "                num_image_tokens = prompt.count(replace_token) * model.get_vision_tower().num_patches\n",
    "            else:\n",
    "                images = None\n",
    "            image_args = {\"images\": images}\n",
    "        else:\n",
    "            images = None\n",
    "            image_args = {}\n",
    "\n",
    "        temperature = float(params.get(\"temperature\", 1.0))\n",
    "        top_p = float(params.get(\"top_p\", 1.0))\n",
    "        max_context_length = getattr(model.config, 'max_position_embeddings', 2048)\n",
    "        max_new_tokens = min(int(params.get(\"max_new_tokens\", 256)), 1024)\n",
    "        stop_str = params.get(\"stop\", \"</s>\")\n",
    "        do_sample = True if temperature > 0.001 else False\n",
    "\n",
    "        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(self.device)\n",
    "        keywords = [stop_str]\n",
    "        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "        max_new_tokens = min(max_new_tokens, max_context_length - input_ids.shape[-1] - num_image_tokens)\n",
    "\n",
    "        return model.generate(\n",
    "            inputs=input_ids,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "            use_cache=True,\n",
    "            **image_args\n",
    "        )\n",
    "    \n",
    "    def construct_target_indexs(self, input_ids, response_token_ids):\n",
    "        b, seq_len = input_ids.shape\n",
    "        select_feature = self.model.get_vision_tower().select_feature\n",
    "        num_vis_tokens = self.model.get_vision_tower().num_patches if select_feature != 'cls_patch' else self.model.get_vision_tower().num_patches + 1\n",
    "        batch_token_index, vis_token_index = torch.where(input_ids == IMAGE_TOKEN_INDEX)\n",
    "        vis_token_index = vis_token_index\n",
    "        assert torch.all(batch_token_index == torch.arange(b)), 'only support each instance containing one image '\n",
    "        print('vision input start location', vis_token_index)\n",
    "        # under a batch style\n",
    "        input_ids_unsqueezed = input_ids.unsqueeze(2)\n",
    "        response_token_ids_unsqueezed = response_token_ids.unsqueeze(1)\n",
    "        batch_indexs, token_indexs = torch.any(input_ids_unsqueezed == response_token_ids_unsqueezed, dim=2).nonzero(as_tuple=True)\n",
    "        token_indexs = token_indexs - 1 # what causes the model to generate the response tokens?\n",
    "        token_indexs = torch.where(token_indexs > vis_token_index, token_indexs + num_vis_tokens - 1, token_indexs)\n",
    "        from_indexs = (batch_indexs, token_indexs)\n",
    "    \n",
    "        to_indexs_vis = (torch.repeat_interleave(batch_token_index, num_vis_tokens), torch.arange(num_vis_tokens).unsqueeze(0).add(vis_token_index.repeat_interleave(b)).mT.flatten())\n",
    "        # print( (input_ids == input_ids).nonzero(as_tuple=True))\n",
    "        # print( (input_ids == input_ids).shape)\n",
    "        batch_indexs, token_indexs = (input_ids == input_ids).nonzero(as_tuple=True)\n",
    "        token_indexs = torch.where(token_indexs > vis_token_index, token_indexs + num_vis_tokens - 1, token_indexs)\n",
    "        to_indexs_text = (batch_indexs, token_indexs)\n",
    "        \n",
    "        return from_indexs, to_indexs_vis, to_indexs_text\n",
    "\n",
    "    # only single round analyse, supports \n",
    "    def analyse(self, query, images, deep_layer=-1, shallow_layer=0, mode='multiple'):\n",
    "        tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor\n",
    "        \n",
    "        # using conversation_lib to preprocess the text input\n",
    "        conv = conversation_lib.conv_llava_v1.copy()\n",
    "        user_input, assistant_output, target_text = query\n",
    "        conv.append_message(conv.roles[0], (user_input, images[0]))\n",
    "        conv.append_message(conv.roles[1], assistant_output)\n",
    "        prompt = conv.get_prompt()\n",
    "        print('model input:', prompt)\n",
    "    \n",
    "        # \n",
    "        response_token_ids = tokenizer(target_text, return_tensors='pt', add_special_tokens=False).input_ids\n",
    "        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0)\n",
    "        \n",
    "        # torch.cuda.empty_cache()\n",
    "        lm_attentions, vision_attentions = self.run4attention(\n",
    "            input_ids.to(device),\n",
    "            images,\n",
    "        )\n",
    "        \n",
    "        # lm_attentions = [attention.to(dtype=torch.float32) for attention in lm_attentions]\n",
    "        # vision_attentions = [attention.to(dtype=torch.float32) for attention in vision_attentions]\n",
    "        attentions = list(itertools.chain(lm_attentions, vision_attentions))\n",
    "        \n",
    "        first_attention = lm_attentions[0]\n",
    "        b, first_num_heads, from_seq_len, to_seq_len = first_attention.shape\n",
    "        \n",
    "        print(to_seq_len)\n",
    "        \n",
    "        from_indexs, to_indexs_vis, to_indexs_text = self.construct_target_indexs(input_ids, response_token_ids)\n",
    "        \n",
    "        start_energy = torch.zeros((b, to_seq_len)).to(first_attention.device, first_attention.dtype)\n",
    "        start_energy[from_indexs] = 1\n",
    "        start_energy = start_energy.unsqueeze(1) # for batch process\n",
    "        start_energy = start_energy / start_energy.sum(2, keepdim=True)\n",
    "\n",
    "        assert deep_layer > shallow_layer or deep_layer + shallow_layer < 0, ''\n",
    "        print(f'attention analysing in layers:deep_layer{deep_layer} - shallow_layer{shallow_layer}')\n",
    "\n",
    "        if mode == 'multiple':\n",
    "            lm_attentions = lm_attentions[deep_layer:shallow_layer:-1]\n",
    "        elif mode == 'average':\n",
    "            lm_attentions = lm_attentions[deep_layer:shallow_layer:-1]\n",
    "            lm_attentions = [torch.stack(lm_attentions).mean(0)]\n",
    "            \n",
    "        energies = self.analyse_attention(start_energy, lm_attentions)\n",
    "        vis_energies, all_energies = [], []\n",
    "        # layers_instances_vis_pils = []\n",
    "        # layers_instances_text_pils = []\n",
    "        for i, energy in enumerate(energies):\n",
    "            energy = energy[:, 0, :] # get rid of the vector dim  \n",
    "            # print(\"sum to one\", energy.sum(1)) # sum to one\n",
    "            # mean, std = energy.mean(1, keepdim=True), energy.std(1, keepdim=True)\n",
    "            # min_, max_ = mean-3*std, mean+3*std\n",
    "            # print(min_, max_)\n",
    "            # num_vis_tok = (to_indexs_vis[0]==0).sum()\n",
    "            # vis_energy = [energy[i, to_indexs_vis[1][i * num_vis_tok],...] for i in range(b)]\n",
    "            vis_energy = energy[:, to_indexs_vis[1]]\n",
    "            text_energy = energy\n",
    "\n",
    "            vis_energies.append(vis_energy)\n",
    "            all_energies.append(text_energy)\n",
    "\n",
    "        return vis_energies, all_energies\n",
    "\n",
    "\n",
    "# print(args.model_path)\n",
    "# analyser = Analyser(args)\n",
    "# analyser.load_from_args()\n",
    "# model, tokenizer, image_processor = analyser.model, analyser.tokenizer, analyser.image_processor\n",
    "\n",
    "# temp_tokenizer = copy.deepcopy(tokenizer)\n",
    "# temp_tokenizer.add_tokens(\"<image>\")\n",
    "# image_token_id = temp_tokenizer.added_tokens_encoder['<image>']\n",
    "analyser = Analyser(args)\n",
    "analyser.set_modules(model, temp_tokenizer, image_processor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse with Model Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Da Vinci '],\n",
    "    [\"Base on this input image, tell me where it might been shot?\", '', ''],\n",
    "]\n",
    "images = ['https://llava-vl.github.io/static/images/monalisa.jpg', 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query, image in zip(queries, images):\n",
    "    res = analyser.generate(query, [image])\n",
    "    res = torch.where(res == -200, image_token_id, res)\n",
    "    print(temp_tokenizer.batch_decode(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse LM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = torch.Tensor([1,2,3]).to(dtype=torch.uint8) \n",
    "# temp = temp * 257 # over flows\n",
    "# print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.arange(12).reshape(3,4)\n",
    "print(temp[temp.nonzero()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.min(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps\n",
    "\n",
    "def convert_pilgray2pyplotpil(img_pil):\n",
    "    from matplotlib import colormaps\n",
    "    cm = plt.get_cmap('viridis')\n",
    "    img_np = np.array(img_pil)\n",
    "    img_np = cm(img_np)\n",
    "    img_pil = Image.fromarray((img_np[:, :, :3] * 255).astype(np.uint8), 'RGB')\n",
    "    return img_pil\n",
    "    \n",
    "from matplotlib import pyplot as plt\n",
    "def visualize_spatial_energy(energy, shape=None):\n",
    "    b, seq_len = energy.shape\n",
    "    if shape is None:\n",
    "        l = math.isqrt(seq_len)\n",
    "        shape = (l, l)\n",
    "        \n",
    "    if math.prod(shape) != seq_len:\n",
    "        raise ValueError('')\n",
    "\n",
    "    energy = energy.reshape(b, *shape)\n",
    "    # def to_pil_image(img):\n",
    "    #     # basically this is what the to_pil_image in torch looks like for a 1 channel image\n",
    "    #     # print(img.shape)\n",
    "    #     # print(type(img))\n",
    "    #     # print(img.dtype)\n",
    "    #     # min_, max_ = img.min(), img.max()\n",
    "    #     # img = (img - min_) / (min_ - max_)\n",
    "    #     if img.ndimension()==2:\n",
    "    #         img = img.unsqueeze(0)\n",
    "    #     img = img.mul(255).byte()\n",
    "    #     img = np.transpose(img.cpu().numpy(), (1, 2, 0))\n",
    "    #     img = img[..., 0]\n",
    "    #     print(img)\n",
    "    #     img = Image.fromarray(img, mode='L')\n",
    "    #     return img\n",
    "        \n",
    "    energy_map_pils = [convert_pilgray2pyplotpil(to_pil_image(e.clip(0, 1))) for e in energy]    \n",
    "    return energy_map_pils\n",
    "\n",
    "\n",
    "def visualize_1d_energy(energy):\n",
    "\n",
    "    b, seq_len = energy.shape\n",
    "    energy = energy.reshape(b, 1, seq_len)\n",
    "    energy_map_pils = [convert_pilgray2pyplotpil(to_pil_image(e.clip(0, 1))) for e in energy]   \n",
    "    return energy_map_pils\n",
    "\n",
    "def replace_batch_zero_with_nonzero_min(energy, nonzero_min):\n",
    "    energy = torch.where(energy == 0, torch.finfo(energy.dtype).max, energy)\n",
    "    energy = torch.where(energy == torch.finfo(energy.dtype).max, nonzero_min, energy)\n",
    "    return energy\n",
    "    \n",
    "def analyse_gap(analyser, queries, images, start, gap, mode='average'):\n",
    "    for query, imgs in zip(queries, images):\n",
    "        print('=' * 200)\n",
    "        current = start\n",
    "        while current - gap > 0:\n",
    "            print('-' * 200)\n",
    "            print(query)\n",
    "            print(imgs)\n",
    "            current = current - gap\n",
    "            deep_layer = current + gap\n",
    "            shallow_layer = current\n",
    "            # analyser.analyse(query, imgs, deep_layer, shallow_layer, mode=mode)\n",
    "            \n",
    "            vis_energies, all_energies = analyser.analyse(query, imgs, deep_layer, shallow_layer, mode=mode)\n",
    "            \n",
    "            layers_instances_vis_pils, layers_instances_text_pils = [], []\n",
    "            for vis_energy, energy in zip(vis_energies, all_energies):\n",
    "                # fill with the none zero min\n",
    "                temp = torch.where(energy == 0, torch.finfo(energy.dtype).max, energy)\n",
    "\n",
    "                nonzero_min = temp.min(1, keepdim=True)[0]\n",
    "                energy = replace_batch_zero_with_nonzero_min(energy, nonzero_min)\n",
    "                vis_energy = replace_batch_zero_with_nonzero_min(vis_energy, nonzero_min)\n",
    "                energy = energy.log()\n",
    "                vis_energy = vis_energy.log()\n",
    "                mean, std = energy.mean(1, keepdim=True), energy.std(1, keepdim=True)\n",
    "                min_, max_ = mean-3*std, mean+3*std\n",
    "                # min_, _ = torch.min(energy, keepdim=True, dim=1)\n",
    "                # max_, _ = torch.max(energy, keepdim=True, dim=1)\n",
    "                energy = (energy - min_) / (max_ - min_)\n",
    "                vis_energy = (vis_energy - min_) / (max_ - min_) \n",
    "                energy = energy.clip(0, 1)\n",
    "                vis_energy = vis_energy.clip(0, 1)\n",
    "                \n",
    "                layer_vis_pils = visualize_spatial_energy(vis_energy)\n",
    "                layer_text_pils = visualize_1d_energy(energy) \n",
    "                layers_instances_vis_pils.append(layer_vis_pils)\n",
    "                layers_instances_text_pils.append(layer_text_pils)\n",
    "            \n",
    "            instances_layers_vis_pils = transpose_list(layers_instances_vis_pils)\n",
    "            instances_layers_text_pils = transpose_list(layers_instances_text_pils)\n",
    "            \n",
    "            display(make_image_grid(instances_layers_vis_pils[0], resize=128)) # index by batch\n",
    "            display(make_image_grid( instances_layers_text_pils[0], cols=1, resize=(2000, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The author of the painting is Leonardo Da Vinci.', 'Leonardo Da Vinci'], # Model Output\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The author of the painting is Claude Monet.', 'Claude Monet'], # Injected Halu\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Da Vinci '], # Made up Output    \n",
    "    \n",
    "]\n",
    "images = [['https://llava-vl.github.io/static/images/monalisa.jpg'],] * len(queries)\n",
    "images = [list(map(load_image, i) ) for i in images]\n",
    "\n",
    "display(images[0][0])\n",
    "analyse_gap(analyser, queries, images, 16, 15, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Da Vinci '],\n",
    "    [\"Base on this input image, tell me where it might been shot?\", '', ''],\n",
    "]\n",
    "images = ['https://llava-vl.github.io/static/images/monalisa.jpg', 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = queries[0]\n",
    "vis_energies, all_energies = analyser.analyse(query, [load_image(image) for image in images[:1]], 32, 0, mode='average')\n",
    "# vis_energies, all_energies = analyser.analyse(query, [load_image(image) for image in images[:1]], 32, 0, mode='multiple')\n",
    "\n",
    "layers_instances_vis_pils, layers_instances_text_pils = [], []\n",
    "for vis_energy, energy in zip(vis_energies, all_energies):\n",
    "    # energy = energy.to(dtype=torch.float32)\n",
    "    # vis_energy = vis_energy.to(dtype=torch.float32)\n",
    "    # ep = energy[energy.nonzero()].min()\n",
    "    # energy = torch.where(energy == 0, ep, energy)\n",
    "    # vis_energy = torch.where(vis_energy == 0, ep, vis_energy)\n",
    "    # energy = energy.log()\n",
    "    # vis_energy = vis_energy.log()\n",
    "    mean, std = energy.mean(1, keepdim=True), energy.std(1, keepdim=True)\n",
    "    min_, max_ = mean-3*std, mean+3*std\n",
    "    # min_, _ = torch.min(energy, keepdim=True, dim=1)\n",
    "    # max_, _ = torch.max(energy, keepdim=True, dim=1)\n",
    "    # energy = energy.clip(min_, max_)\n",
    "    # vis_energy = vis_energy.clip(min_, max_)\n",
    "    energy = (energy - min_) / (max_ - min_)\n",
    "    vis_energy = (vis_energy - min_) / (max_ - min_) \n",
    "    energy = energy.clip(0, 1)\n",
    "    vis_energy = vis_energy.clip(0, 1)\n",
    "    \n",
    "    vis_energy = vis_energy * vis_energy.shape[1] * 128\n",
    "    energy = energy * energy.shape[1] * 128\n",
    "    \n",
    "    layer_vis_pils = visualize_spatial_energy(vis_energy)\n",
    "    layer_text_pils = visualize_1d_energy(energy)\n",
    "    layers_instances_vis_pils.append(layer_vis_pils)\n",
    "    layers_instances_text_pils.append(layer_text_pils)\n",
    "\n",
    "instances_layers_vis_pils = transpose_list(layers_instances_vis_pils)\n",
    "instances_layers_text_pils = transpose_list(layers_instances_text_pils)\n",
    "\n",
    "display(make_image_grid(instances_layers_vis_pils[0], resize=128)) # index by batch\n",
    "display(make_image_grid( instances_layers_text_pils[0], cols=1, resize=(2000, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyse_gap(analyser, queries, images, start, gap, mode='average'):\n",
    "#     for query, imgs in zip(queries, images):\n",
    "#         print('=' * 200)\n",
    "#         current = start\n",
    "#         while current - gap > 0:\n",
    "#             print('-' * 200)\n",
    "#             print(query)\n",
    "#             print(imgs)\n",
    "#             current = current - gap\n",
    "#             deep_layer = current + gap\n",
    "#             shallow_layer = current\n",
    "#             # analyser.analyse(query, imgs, deep_layer, shallow_layer, mode=mode)\n",
    "            \n",
    "#             vis_energies, all_energies = analyser.analyse(query, imgs, deep_layer, shallow_layer, mode=mode)\n",
    "            \n",
    "#             layers_instances_vis_pils, layers_instances_text_pils = [], []\n",
    "#             for vis_energy, energy in zip(vis_energies, all_energies):\n",
    "#                 mean, std = energy.mean(1, keepdim=True), energy.std(1, keepdim=True)\n",
    "#                 min_, max_ = mean-3*std, mean+3*std\n",
    "#                 layer_vis_pils = visualize_spatial_energy(vis_energy, min_=min_, max_=max_)\n",
    "#                 layer_text_pils = visualize_1d_energy(energy, min_=min_, max_=max_)\n",
    "#                 layers_instances_vis_pils.append(layer_vis_pils)\n",
    "#                 layers_instances_text_pils.append(layer_text_pils)\n",
    "            \n",
    "#             instances_layers_vis_pils = transpose_list(layers_instances_vis_pils)\n",
    "#             instances_layers_text_pils = transpose_list(layers_instances_text_pils)\n",
    "            \n",
    "#             display(make_image_grid(instances_layers_vis_pils[0], resize=128)) # index by batch\n",
    "#             display(make_image_grid( instances_layers_text_pils[0], cols=1, resize=(2000, 20)))\n",
    "# def visualize_spatial_energy(energy, min_, max_, shape=None):\n",
    "#     b, seq_len = energy.shape\n",
    "#     if shape is None:\n",
    "#         l = math.isqrt(seq_len)\n",
    "#         shape = (l, l)\n",
    "        \n",
    "#     if math.prod(shape) != seq_len:\n",
    "#         raise ValueError('')\n",
    "\n",
    "#     energy = (energy - min_) * 255 / (max_ - min_)\n",
    "#     energy = energy.clip(0, 255)\n",
    "#     energy = energy.reshape(b, *shape)\n",
    "#     # energy = energy.expand(2, -1, -1).unsqueeze(1)\n",
    "#     # print(energy.shape)\n",
    "    \n",
    "#     energy_map_pils = [to_pil_image(e) for e in energy]\n",
    "    \n",
    "#     return energy_map_pils\n",
    "\n",
    "# def visualize_1d_energy(energy, min_, max_):\n",
    "\n",
    "#     b, seq_len = energy.shape\n",
    "        \n",
    "#     energy = (energy - min_) * 255 / (max_ - min_)\n",
    "#     energy = energy.clip(0, 255)\n",
    "#     energy = energy.reshape(b, 1, seq_len)\n",
    "    \n",
    "#     energy_map_pils = [to_pil_image(e) for e in energy]\n",
    "    \n",
    "#     return energy_map_pils\n",
    "\n",
    "# queries = [\n",
    "#     ['Base on this input image, tell me who is the author of the painting?', 'The author of the painting is Leonardo Da Vinci.', 'Da Vinci'], # Model Output\n",
    "#     ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Da Vinci '], # Made up Output    \n",
    "#     ['Base on this input image, tell me who is the author of the painting?', 'The author of the painting is Leonardo Monnet.', 'Monnet'], # Injected Halu\n",
    "    \n",
    "# ]\n",
    "# images = [['https://llava-vl.github.io/static/images/monalisa.jpg'],] * len(queries)\n",
    "# images = [list(map(load_image, i) ) for i in images]\n",
    "\n",
    "# # display(images[0][0])\n",
    "# analyse_gap(analyser, queries, images, 32, 16, mode='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More hacked Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    ('what is this?', 'Monalisa', 'Monalisa'),\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Da Vinci '],\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Monalisa '],    \n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Monnet', 'Monnet '],\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Monnet', 'Monalisa '],\n",
    "]\n",
    "images = [['https://llava-vl.github.io/static/images/monalisa.jpg'],] * len(queries)\n",
    "images = [list(map(load_image, i) ) for i in images]\n",
    "\n",
    "# display(images[0][0])\n",
    "analyse_gap(analyser, queries, images, 16, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(images[0][0])\n",
    "analyse_gap(analyser, queries, images, 16, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = 'In this case, the white four-door pickup truck seems parked on a street in a downtown city area. Specifically at a street corner near the edge of a red building. There is a sidewalk next to the parked truck, and a potted plant is visible nearby as well.',\n",
    "# queries = [\n",
    "#     [\n",
    "#         'Base on this input image, tell me where it might been shot?', \n",
    "#         res,\n",
    "#         'truck'\n",
    "#      ],\n",
    "#     ['Base on this input image, tell me where it might been shot?', 'The photo contains a car, it might been shot on a street', 'car'],\n",
    "#     ['Base on this input image, tell me where it might been shot?', 'The photo contains a truck, it might been shot on a street', 'truck'],\n",
    "#     ['Base on this input image, tell me where it might been shot?', 'The photo contains a bicycle, it might been shot on a street', 'bicycle'],\n",
    "#     ['Base on this input image, tell me where it might been shot?', 'The photo contains a truck, it might been shot on a street', 'street'],\n",
    "# ]\n",
    "# images = [['https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'],] * len(queries)\n",
    "# images = [list(map(load_image, i) ) for i in images]\n",
    "# # display(images[0][0])\n",
    "# analyse_gap(analyser, queries, images, 16, 5, mode='multiple')\n",
    "\n",
    "question = 'Base on this input image, tell me where it might been shot?'\n",
    "res = 'In this case, the white four-door pickup truck seems parked on a street in a downtown city area. Specifically at a street corner near the edge of a red building. There is a sidewalk next to the parked truck, and a potted plant is visible nearby as well.'\n",
    "queries = [\n",
    "    [question, res, 'truck'], # Model Output    \n",
    "    [question, res, 'potted plant'], # Model Output    \n",
    "]\n",
    "images = [['https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'],] * len(queries)\n",
    "images = [list(map(load_image, i) ) for i in images]\n",
    "\n",
    "# display(images[0][0])\n",
    "# analyse_gap(analyser, queries, images, 16, 3, mode='multiple')\n",
    "analyse_gap(analyser, queries, images, 32, 5, mode='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyse vision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model = model.get_vision_tower()\n",
    "vision_model.select_feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
