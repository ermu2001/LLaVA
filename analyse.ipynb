{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union, List\n",
    "import copy\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "import transformers\n",
    "\n",
    "from llava.mm_utils import process_images, load_image_from_base64, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.mm_utils import process_images, load_image_from_base64, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "\n",
    "\n",
    "from attention_analyse import (\n",
    "    load_image,\n",
    "    transpose_list,\n",
    "    make_image_grid,\n",
    "    visualize_1d_energy,\n",
    "    visualize_spatial_energy\n",
    ")\n",
    "device='cuda:1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ARGS():\n",
    "    model_path=\"liuhaotian/llava-v1.5-7b\"\n",
    "    model_base=None\n",
    "    model_name=None\n",
    "    device=device\n",
    "    multi_modal=True\n",
    "    load_8bit=False\n",
    "    load_4bit=False\n",
    "args = ARGS() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyser():\n",
    "    is_multimodal=True\n",
    "    def __init__(self, args):\n",
    "        self.set_args(args)\n",
    "        \n",
    "    def set_args(self, args):\n",
    "        self.args = args\n",
    "        model_path = args.model_path\n",
    "        if model_path.endswith(\"/\"):\n",
    "            model_path = model_path[:-1]\n",
    "\n",
    "        if args.model_name is None:\n",
    "            model_paths = model_path.split(\"/\")\n",
    "            if model_paths[-1].startswith('checkpoint-'):\n",
    "                self.model_name = model_paths[-2] + \"_\" + model_paths[-1]\n",
    "            else:\n",
    "                self.model_name = model_paths[-1]\n",
    "        else:\n",
    "            self.model_name = args.model_name\n",
    "        self.device = self.args.device\n",
    "    \n",
    "    def load_from_args(self):\n",
    "        model_path = self.args.model_path\n",
    "        model_base = self.args.model_base\n",
    "        load_8bit = self.args.load_8bit\n",
    "        load_4bit = self.args.load_4bit\n",
    "        model_name = self.model_name\n",
    "        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\n",
    "            model_path, model_base, model_name, load_8bit, load_4bit, device=self.device)\n",
    "        self.is_multimodal = 'llava' in self.model_name.lower()\n",
    "\n",
    "    def set_modules(self, model=None, tokenizer=None, image_processor=None):\n",
    "        self.model = model if model is not None else self.model\n",
    "        self.tokenizer = tokenizer if tokenizer is not None else self.tokenizer\n",
    "        self.image_processor = image_processor if image_processor is not None else self.image_processor\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def analyse_attention(self, start_energy, attentions):\n",
    "        from_seq_len = start_energy.shape[2]\n",
    "        # energy_transforme_matrix = torch.diag(torch.ones(to_seq_len))\n",
    "        # energy_transforme_matrix = torch.unsqueeze(energy_transforme_matrix, 0)\n",
    "        # print(energy_transforme_matrix.shape)\n",
    "        # return\n",
    "        energies = []\n",
    "        energy = start_energy\n",
    "        for layer_attention in attentions:\n",
    "            layer_attention = layer_attention.mean(1) # mean over multi heads\n",
    "            if layer_attention.shape[1] != from_seq_len:\n",
    "                raise ValueError('')\n",
    "            \n",
    "            # print(layer_attention.shape)\n",
    "            # print(layer_attention.sum(2)) # all one\n",
    "            # print(energy.shape)\n",
    "            # print(energy.sum(1))\n",
    "            energy = energy @ layer_attention\n",
    "            energies.append(energy)\n",
    "        return energies\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def run4attention(self, input_ids, images):\n",
    "        tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor\n",
    "        images = process_images(images, image_processor, model.config)\n",
    "        if type(images) is list:\n",
    "            images = [image.to(self.model.device, dtype=torch.float16) for image in images]\n",
    "        else:\n",
    "            images = images.to(self.model.device, dtype=torch.float16)\n",
    "    \n",
    "        lm_model_out = model(\n",
    "            input_ids=input_ids,\n",
    "            images=images,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "        lm_attentions = lm_model_out.attentions\n",
    "        \n",
    "        vision_tower = model.get_vision_tower().vision_tower\n",
    "        vision_model_out = vision_tower(images, output_attentions=True)\n",
    "        vision_attentions = vision_model_out.attentions\n",
    "        return lm_attentions, vision_attentions\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, query, images, **params):\n",
    "        tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor\n",
    "        \n",
    "        # using conversation_lib to preprocess the text input\n",
    "        conv = conversation_lib.conv_llava_v1.copy()\n",
    "        user_input, assistant_output, target_text = query\n",
    "        conv.append_message(conv.roles[0], (user_input, images[0]))\n",
    "        # conv.append_message(conv.roles[1], assistant_output)\n",
    "        prompt = conv.get_prompt()\n",
    "        ori_prompt = prompt\n",
    "        num_image_tokens = 0\n",
    "        if images is not None and len(images) > 0 and self.is_multimodal:\n",
    "            if len(images) > 0:\n",
    "                if len(images) != prompt.count(DEFAULT_IMAGE_TOKEN):\n",
    "                    raise ValueError(\"Number of images does not match number of <image> tokens in prompt\")\n",
    "\n",
    "                images = [load_image(image) for image in images]\n",
    "                images = process_images(images, image_processor, model.config)\n",
    "\n",
    "                if type(images) is list:\n",
    "                    images = [image.to(self.model.device, dtype=torch.float16) for image in images]\n",
    "                else:\n",
    "                    images = images.to(self.model.device, dtype=torch.float16)\n",
    "\n",
    "                replace_token = DEFAULT_IMAGE_TOKEN\n",
    "                if getattr(self.model.config, 'mm_use_im_start_end', False):\n",
    "                    replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "                prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "\n",
    "                num_image_tokens = prompt.count(replace_token) * model.get_vision_tower().num_patches\n",
    "            else:\n",
    "                images = None\n",
    "            image_args = {\"images\": images}\n",
    "        else:\n",
    "            images = None\n",
    "            image_args = {}\n",
    "\n",
    "        temperature = float(params.get(\"temperature\", 1.0))\n",
    "        top_p = float(params.get(\"top_p\", 1.0))\n",
    "        max_context_length = getattr(model.config, 'max_position_embeddings', 2048)\n",
    "        max_new_tokens = min(int(params.get(\"max_new_tokens\", 256)), 1024)\n",
    "        stop_str = params.get(\"stop\", \"</s>\")\n",
    "        do_sample = True if temperature > 0.001 else False\n",
    "\n",
    "        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(self.device)\n",
    "        keywords = [stop_str]\n",
    "        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "        max_new_tokens = min(max_new_tokens, max_context_length - input_ids.shape[-1] - num_image_tokens)\n",
    "\n",
    "        return model.generate(\n",
    "            inputs=input_ids,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "            use_cache=True,\n",
    "            **image_args\n",
    "        )\n",
    "    \n",
    "    def construct_target_indexs(self, input_ids, response_token_ids):\n",
    "        b, seq_len = input_ids.shape\n",
    "        select_feature = self.model.get_vision_tower().select_feature\n",
    "        num_vis_tokens = self.model.get_vision_tower().num_patches if select_feature != 'cls_patch' else self.model.get_vision_tower().num_patches + 1\n",
    "        batch_token_index, vis_token_index = torch.where(input_ids == IMAGE_TOKEN_INDEX)\n",
    "        vis_token_index = vis_token_index\n",
    "        assert torch.all(batch_token_index == torch.arange(b)), 'only support each instance containing one image '\n",
    "        print(vis_token_index)\n",
    "        # under a batch style\n",
    "        input_ids_unsqueezed = input_ids.unsqueeze(2)\n",
    "        response_token_ids_unsqueezed = response_token_ids.unsqueeze(1)\n",
    "        batch_indexs, token_indexs = torch.any(input_ids_unsqueezed == response_token_ids_unsqueezed, dim=2).nonzero(as_tuple=True)\n",
    "        token_indexs = torch.where(token_indexs > vis_token_index, token_indexs + num_vis_tokens - 1, token_indexs)\n",
    "        from_indexs = (batch_indexs, token_indexs)\n",
    "    \n",
    "        to_indexs_vis = (torch.repeat_interleave(batch_token_index, num_vis_tokens), torch.arange(num_vis_tokens).unsqueeze(0).add(vis_token_index.repeat_interleave(b)).mT.flatten())\n",
    "        # print( (input_ids == input_ids).nonzero(as_tuple=True))\n",
    "        # print( (input_ids == input_ids).shape)\n",
    "        batch_indexs, token_indexs = (input_ids == input_ids).nonzero(as_tuple=True)\n",
    "        token_indexs = torch.where(token_indexs > vis_token_index, token_indexs + num_vis_tokens - 1, token_indexs)\n",
    "        to_indexs_text = (batch_indexs, token_indexs)\n",
    "        \n",
    "        return from_indexs, to_indexs_vis, to_indexs_text\n",
    "\n",
    "    # only single round analyse, supports \n",
    "    def analyse(self, query, images, deep_layer=-1, shallow_layer=0, mode='multiple'):\n",
    "        tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor\n",
    "        \n",
    "        # using conversation_lib to preprocess the text input\n",
    "        conv = conversation_lib.conv_llava_v1.copy()\n",
    "        user_input, assistant_output, target_text = query\n",
    "        conv.append_message(conv.roles[0], (user_input, images[0]))\n",
    "        conv.append_message(conv.roles[1], assistant_output)\n",
    "        prompt = conv.get_prompt()\n",
    "        print('model input:', prompt)\n",
    "    \n",
    "        # \n",
    "        response_token_ids = tokenizer(target_text, return_tensors='pt', add_special_tokens=False).input_ids\n",
    "        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0)\n",
    "        \n",
    "        # torch.cuda.empty_cache()\n",
    "        lm_attentions, vision_attentions = self.run4attention(\n",
    "            input_ids.to(device),\n",
    "            images,\n",
    "        )\n",
    "        \n",
    "        # lm_attentions = [attention.to(dtype=torch.float32) for attention in lm_attentions]\n",
    "        # vision_attentions = [attention.to(dtype=torch.float32) for attention in vision_attentions]\n",
    "        attentions = list(itertools.chain(lm_attentions, vision_attentions))\n",
    "        \n",
    "        first_attention = lm_attentions[0]\n",
    "        b, first_num_heads, from_seq_len, to_seq_len = first_attention.shape\n",
    "        \n",
    "        print(to_seq_len)\n",
    "        \n",
    "        from_indexs, to_indexs_vis, to_indexs_text = self.construct_target_indexs(input_ids, response_token_ids)\n",
    "        \n",
    "        start_energy = torch.zeros((b, to_seq_len)).to(first_attention.device, first_attention.dtype)\n",
    "        start_energy[from_indexs] = 1\n",
    "        start_energy = start_energy.unsqueeze(1) # for batch process\n",
    "        start_energy = start_energy / start_energy.sum(2, keepdim=True)\n",
    "\n",
    "        assert deep_layer > shallow_layer or deep_layer + shallow_layer < 0, ''\n",
    "        print(f'attention visualizing in layers:deep_layer{deep_layer} - shallow_layer{shallow_layer}')\n",
    "\n",
    "        if mode == 'multiple':\n",
    "            lm_attentions = lm_attentions[deep_layer:shallow_layer:-1]\n",
    "        elif mode == 'average':\n",
    "            lm_attentions = lm_attentions[deep_layer:shallow_layer:-1]\n",
    "            lm_attentions = [torch.stack(lm_attentions).mean(0)]\n",
    "            \n",
    "        energies = self.analyse_attention(start_energy, lm_attentions)\n",
    "        layers_instances_vis_pils = []\n",
    "        layers_instances_text_pils = []\n",
    "        for i, energy in enumerate(energies):\n",
    "            energy = energy[:, 0, :]\n",
    "            # print(\"sum to one\", energy.sum(1)) # sum to one\n",
    "            mean, std = energy.mean(1, keepdim=True), energy.std(1, keepdim=True)\n",
    "            min_, max_ = mean-3*std, mean+3*std\n",
    "            # print(min_, max_)\n",
    "            num_vis_tok = (to_indexs_vis[0]==0).sum()\n",
    "            vis_energy = [energy[i, to_indexs_vis[1][i * num_vis_tok],...] for i in range(b)]\n",
    "            vis_energy = energy[:, to_indexs_vis[1]]\n",
    "            text_energy = energy\n",
    "            layer_vis_pils = visualize_spatial_energy(vis_energy, min_=min_, max_=max_)\n",
    "            layer_text_pils = visualize_1d_energy(text_energy, min_=min_, max_=max_)\n",
    "            layers_instances_vis_pils.append(layer_vis_pils)\n",
    "            layers_instances_text_pils.append(layer_text_pils)\n",
    "        \n",
    "        instances_layers_vis_pils = transpose_list(layers_instances_vis_pils)\n",
    "        instances_layers_text_pils = transpose_list(layers_instances_text_pils)\n",
    "        \n",
    "        display(make_image_grid(instances_layers_vis_pils[0], resize=128))\n",
    "        display(make_image_grid( instances_layers_text_pils[0], cols=1, resize=(2000, 20)))\n",
    "        return energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(args.model_path)\n",
    "# analyser = Analyser(args)\n",
    "# analyser.load_from_args()\n",
    "# model, tokenizer, image_processor = analyser.model, analyser.tokenizer, analyser.image_processor\n",
    "\n",
    "# analyser = Analyser(args)\n",
    "# temp_tokenizer = copy.deepcopy(tokenizer)\n",
    "temp_tokenizer.add_tokens(\"<image>\")\n",
    "image_token_id = temp_tokenizer.added_tokens_encoder['<image>']\n",
    "analyser.set_modules(model, temp_tokenizer, image_processor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse with Model Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Da Vinci ']\n",
    "images = ['https://llava-vl.github.io/static/images/monalisa.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyser.generate(query, images)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.where(res == -200, image_token_id, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tokenizer.batch_decode(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse with Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "energies = analyser.analyse(query, [load_image(image) for image in images], 32, 0, mode='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_gap(analyser, queries, images, start, gap, mode='average'):\n",
    "    for query, imgs in zip(queries, images):\n",
    "        print('=' * 200)\n",
    "        current = start\n",
    "        while current - gap > 0:\n",
    "            print('-' * 200)\n",
    "            print(query)\n",
    "            print(imgs)\n",
    "            current = current - gap\n",
    "            deep_layer = current + gap\n",
    "            shallow_layer = current\n",
    "            analyser.analyse(query, imgs, deep_layer, shallow_layer, mode=mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "queries = [\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The author of the painting is Leonardo Da Vinci.', 'Da Vinci'], # Model Output\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Da Vinci '], # Made up Output    \n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The author of the painting is Leonardo Monnet.', 'Monnet'], # Injected Halu\n",
    "    \n",
    "]\n",
    "images = [['https://llava-vl.github.io/static/images/monalisa.jpg'],] * len(queries)\n",
    "images = [list(map(load_image, i) ) for i in images]\n",
    "\n",
    "# display(images[0][0])\n",
    "analyse_gap(analyser, queries, images, 16, 2, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    ('what is this?', 'Monalisa', 'Monalisa'),\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Da Vinci '],\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Da Vinci', 'Monalisa '],    \n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Monnet', 'Monnet '],\n",
    "    ['Base on this input image, tell me who is the author of the painting?', 'The painting is the famous Monalisa, and the author is Monnet', 'Monalisa '],\n",
    "]\n",
    "images = [['https://llava-vl.github.io/static/images/monalisa.jpg'],] * len(queries)\n",
    "images = [list(map(load_image, i) ) for i in images]\n",
    "\n",
    "# display(images[0][0])\n",
    "analyse_gap(analyser, queries, images, 16, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(images[0][0])\n",
    "analyse_gap(analyser, queries, images, 16, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model = model.get_vision_tower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model.select_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    ('what is this?', 'Car', 'Car'),\n",
    "    ['Base on this input image, tell me where it might been shot?', 'The photo contains a car, it might been shot on a street', 'car'],\n",
    "    ['Base on this input image, tell me where it might been shot?', 'The photo contains a truck, it might been shot on a street', 'truck'],\n",
    "    ['Base on this input image, tell me where it might been shot?', 'The photo contains a bicycle, it might been shot on a street', 'bicycle'],\n",
    "    ['Base on this input image, tell me where it might been shot?', 'The photo contains a truck, it might been shot on a street', 'street'],\n",
    "]\n",
    "images = [['https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'],] * len(queries)\n",
    "images = [list(map(load_image, i) ) for i in images]\n",
    "# display(images[0][0])\n",
    "analyse_gap(analyser, queries, images, 16, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
